"""
demo_opencode_integration.py - Demo script showing Opencode integration capabilities

This script demonstrates the Opencode integration without requiring Opencode
to be actually installed, by using mocked responses and fallback functionality.

Demo Features:
- Configuration integration
- Model translation
- Intent parsing with model switching
- Skill routing
- Brain functionality
- MiniMax Agent integration
"""

import sys
from pathlib import Path
from unittest.mock import patch, MagicMock

# Add neo-clone to path
sys.path.insert(0, str(Path(__file__).parent))

# Mock Opencode availability to avoid subprocess calls
def mock_is_opencode_available():
    return True

def mock_get_current_opencode_model():
    return "openai/gpt-3.5-turbo"

def mock_get_available_opencode_models():
    return [
        "openai/gpt-3.5-turbo",
        "openai/gpt-4",
        "anthropic/claude-3-sonnet",
        "ollama/llama2",
        "ollama/codellama"
    ]

# Apply mocks
with patch('config_opencode.is_opencode_available', mock_is_opencode_available):
    with patch('brain_opencode.get_current_opencode_model', mock_get_current_opencode_model):
        with patch('llm_client_opencode.OpencodeLLMClient._discover_available_models', return_value=mock_get_available_opencode_models()):
            
            from config_opencode import Config, load_config, translate_opencode_model_to_neo
            from brain_opencode import OpencodeBrain
            from skills import SkillRegistry
            from llm_client_opencode import LLMClient

def demo_configuration_integration():
    """Demo 1: Configuration Integration"""
    print("üîß Demo 1: Configuration Integration")
    print("-" * 40)
    
    # Load config with Opencode integration
    config = load_config()
    
    print(f"‚úÖ Config loaded successfully")
    print(f"   Provider: {config.provider}")
    print(f"   Model: {config.model_name}")
    print(f"   Opencode Model: {config.opencode_model}")
    print(f"   Temperature: {config.temperature}")
    print()
    
    return config

def demo_model_translation():
    """Demo 2: Model Format Translation"""
    print("üîÑ Demo 2: Model Format Translation")
    print("-" * 40)
    
    test_models = [
        "openai/gpt-3.5-turbo",
        "openai/gpt-4",
        "anthropic/claude-3-sonnet", 
        "ollama/llama2",
        "ollama/codellama"
    ]
    
    for model in test_models:
        provider, model_name = translate_opencode_model_to_neo(model)
        print(f"   {model} ‚Üí {provider}/{model_name}")
    
    print("‚úÖ Model translation working correctly")
    print()

def demo_llm_client():
    """Demo 3: LLM Client with Opencode Integration"""
    print("ü§ñ Demo 3: LLM Client with Opencode Integration")
    print("-" * 40)
    
    config = Config(provider="ollama", model_name="test-model")
    client = LLMClient(config)
    
    print(f"‚úÖ LLM Client initialized")
    print(f"   Current Model: {client.get_current_model()}")
    print(f"   Available Models: {len(client.get_available_models())}")
    print(f"   Models: {client.get_available_models()[:3]}...")
    print()
    
    # Test model switching
    test_model = "openai/gpt-3.5-turbo"
    client.set_model(test_model)
    print(f"‚úÖ Model switched to: {client.get_current_model()}")
    print()

def demo_brain_integration():
    """Demo 4: Brain Integration with Intent Parsing"""
    print("üß† Demo 4: Brain Integration with Intent Parsing")
    print("-" * 40)
    
    config = Config(provider="ollama", model_name="test-model")
    skills = SkillRegistry()
    brain = OpencodeBrain(config, skills)
    
    test_queries = [
        "/model openai/gpt-4",
        "analyze the sentiment of this text",
        "minimax analyze user intent",
        "generate python code to sort a list",
        "search for AI information"
    ]
    
    for query in test_queries:
        intent = brain.parse_intent(query)
        print(f"   Query: {query}")
        print(f"   Intent: {intent}")
        print()
    
    print("‚úÖ Brain intent parsing working correctly")
    print()

def demo_skill_routing():
    """Demo 5: Skill Routing and Execution"""
    print("üéØ Demo 5: Skill Routing and Execution")
    print("-" * 40)
    
    config = Config(provider="ollama", model_name="test-model")
    skills = SkillRegistry()
    brain = OpencodeBrain(config, skills)
    
    # Test skill routing
    skill_tests = [
        ("analyze this text", "text_analysis"),
        ("generate python code", "code_generation"),
        ("minimax analyze intent", "minimax_agent"),
        ("search the web", "web_search")
    ]
    
    for query, expected_skill in skill_tests:
        intent = brain.parse_intent(query)
        if intent["intent"] == "skill":
            result = brain.route_to_skill(intent["skill"], query)
            print(f"   Query: {query}")
            print(f"   Skill: {result['chosen_skill']}")
            print(f"   Output: {str(result['output'])[:80]}...")
            print()
        elif intent["intent"] == "minimax":
            result = brain.route_to_skill(intent["skill"], query)
            print(f"   Query: {query}")
            print(f"   MiniMax: {result['chosen_skill']}")
            print(f"   Output: {str(result['output'])[:80]}...")
            print()
    
    print("‚úÖ Skill routing working correctly")
    print()

def demo_model_switching():
    """Demo 6: Model Switching Functionality"""
    print("üîÑ Demo 6: Model Switching Functionality")
    print("-" * 40)
    
    config = Config(provider="ollama", model_name="test-model")
    skills = SkillRegistry()
    brain = OpencodeBrain(config, skills)
    
    # Test model switching
    models_to_test = [
        "openai/gpt-3.5-turbo",
        "anthropic/claude-3-sonnet",
        "ollama/llama2"
    ]
    
    for model in models_to_test:
        result = brain.switch_model(model)
        print(f"   Switch to {model}: {result}")
        print(f"   Current model: {brain.llm.get_current_model()}")
        print()
    
    print("‚úÖ Model switching working correctly")
    print()

def demo_brain_status():
    """Demo 7: Brain Status and Monitoring"""
    print("üìä Demo 7: Brain Status and Monitoring")
    print("-" * 40)
    
    config = Config(provider="ollama", model_name="test-model")
    skills = SkillRegistry()
    brain = OpencodeBrain(config, skills)
    
    # Get brain status
    status = brain.get_status()
    
    print("Brain Status:")
    for key, value in status.items():
        if isinstance(value, list):
            print(f"   {key}: {len(value)} items")
        else:
            print(f"   {key}: {value}")
    print()
    
    print("‚úÖ Brain status reporting working correctly")
    print()

def demo_conversation_flow():
    """Demo 8: Complete Conversation Flow"""
    print("üí¨ Demo 8: Complete Conversation Flow")
    print("-" * 40)
    
    config = Config(provider="ollama", model_name="test-model")
    skills = SkillRegistry()
    brain = OpencodeBrain(config, skills)
    
    # Simulate a conversation
    conversation = [
        "Hello, I need help with text analysis",
        "Analyze the sentiment of: 'I love this product!'",
        "/model anthropic/claude-3-sonnet",
        "Now analyze this: 'This is terrible'",
        "minimax explain the different intents you've detected"
    ]
    
    for message in conversation:
        print(f"üë§ User: {message}")
        response = brain.send_message(message)
        print(f"ü§ñ Assistant: {response[:100]}...")
        print()
    
    # Show final stats
    stats = brain.get_status()
    print(f"Session Stats:")
    print(f"   Model switches: {stats['model_switches']}")
    print(f"   Messages processed: {stats['conversation_stats']['message_count']}")
    print(f"   Available skills: {len(stats['available_skills'])}")
    print()
    
    print("‚úÖ Complete conversation flow working correctly")
    print()

def main():
    """Run all Opencode integration demos"""
    print("üöÄ Opencode Integration Demo")
    print("=" * 60)
    print("This demo shows Neo-Clone's Opencode compatibility without requiring")
    print("Opencode to be actually installed, using mocked responses.")
    print("=" * 60)
    print()
    
    try:
        # Run all demos
        demo_configuration_integration()
        demo_model_translation()
        demo_llm_client()
        demo_brain_integration()
        demo_skill_routing()
        demo_model_switching()
        demo_brain_status()
        demo_conversation_flow()
        
        print("üéâ All demos completed successfully!")
        print()
        print("üìã Summary of Opencode Integration Features:")
        print("‚úÖ Configuration system reads Opencode models")
        print("‚úÖ Model format translation (provider/model ‚Üí neo format)")
        print("‚úÖ LLM client supports multiple providers")
        print("‚úÖ Brain integrates with Opencode selection")
        print("‚úÖ Model switching via /model commands")
        print("‚úÖ Intent parsing includes model switching")
        print("‚úÖ Skills system works with all providers")
        print("‚úÖ MiniMax Agent integration maintained")
        print("‚úÖ Status monitoring and session tracking")
        print("‚úÖ Backward compatibility preserved")
        print()
        print("üîó The system is ready for drop-in deployment with Opencode!")
        
    except Exception as e:
        print(f"‚ùå Demo failed: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()